---
layout:     post
title:      A Transparent Control-ﬂow Based Approach to Record-Replay Non-deterministic Bugs
subtitle:   (CNAS'12)
date:       2020-07-07
author:     qzwerty
header-img: img/blog-bj-1.png
catalog: true
tags:
    - RR security


---

## A Transparent Control-ﬂow Based Approach to Record-Replay Non-deterministic Bugs

### Abstract

记录回放是再现非确定性错误的有效方法，已引起了学术界的广泛关注。然而，由于一些原因，目前的方法无法处理多处理器平台和分布式系统中的不确定性错误。首先，多处理器平台上的多线程程序(在当今的分布式系统中很常见)由于数据竞争，很难被记录和重放。第二，系统规模的增加使得生产环境对记录的干扰更加敏感。甚至黑客控制脚本都是不可接受的，因为各种各样的程序和大量的计算核心增加了复杂性。第三，当在分布式系统中部署时，大规模的记录也会成倍增加，这会让开发人员不知所措，也会极大地降低整个系统的速度。

针对上述问题，本文提出了基于控制流的记录重放、低扰动加载和比例采样等多处理器分布式系统中有效的记录应答机制。我们在ReBranch中实现了这些机制，ReBranch是一个实用的记录回放系统，用于在多处理器平台和分布式系统中调试多线程程序。ReBranch在处理真正的bug方面已经显示了它的威力。我们还通过一个处理memcached(许多商业系统中的重要组件)中的bug的案例研究展示了我们使用ReBranch的调试经验。

**Keywords**: debugging; record-replay; parallel programming; distributed systems;

### I. I NTRODUCTION

bug的复现是调试过程中的第一步也是关键的一步。然而，随着多处理器平台和分布式系统的迅速出现，开发人员面临着越来越多的难以重现的错误。这些bug被称为heisenbugs[1]来表示它们的非确定性。它们通常由无法控制的事件触发，例如数据竞争、消息排序和调度。其中一些只在生产运行时发生。之前的研究[2]表明，在处理并发bug时(大多数是heisenbug)，重现一个bug的时间占了整个调试过程的绝大部分。

```
Heisenbug是计算机编程术语，指当人们试图研究它时，似乎会消失或改变其行为的软件bug。这个术语是维尔纳·海森堡的双关语，这位物理学家首先提出了量子力学的观察者效应，即观察一个系统的行为不可避免地会改变它的状态。
```

确定不确定性bug的根源的常用方法是手动检查日志和分析源代码。然而，日志挖掘是劳动密集型和耗时的。此外，分析细粒度的日志超出了我们的能力，而粗粒度的日志牺牲了大量有用的信息。

记录回放[3]，[4]是一种很有前途的方法，它允许开发人员重现heisenbugs。recordreplay的原理是在执行过程中记录非确定性，并在回放过程中重新创建它们的效果。日志挖掘需要开发人员在头脑中重新执行程序，与此不同的是，在重播期间，目标程序是在现实中重新执行的，因此大多数程序状态可以重新创建。

对于单线程程序以及在单处理器平台上运行的多线程程序，记录播放机制已经被深入研究。对于这些程序，所有的非确定性都来自于操作系统，因此，记录所有的系统调用和另外记录调度顺序工作得很好。

然而，对于多线程程序和分布式系统中记录回放的非确定性错误，仍然存在一些障碍。

首先，在现代系统中，多处理器平台中的多线程程序很常见，但是由于数据竞争，它们很难被精确地记录和回放。从线程的角度来看，所有内存访问都可能是非确定性的。记录所有的内存访问是非常昂贵的，而过滤无害访问是一个NP-hard问题[10]。

第二，在生产系统中，记录的透明度是一个大问题。复杂系统承受不起大扰动。无论额外的硬件，有时甚至引入显式加载器都是困难的，因为控制脚本的复杂性。然而，就我们所知，现有的方法很少能提供这样的透明度。

此外，在处理分布式系统时，应该有一种机制来对执行进行抽样跟踪。否则，开发人员将被大量的跟踪数据所淹没，整个系统的性能也将大大降低。

本文旨在通过我们提出的方法——ReBranch来解决上述三个问题。ReBranch是一种记录播放系统，旨在部署在分布式系统中，重现多线程程序中的非确定性错误。设计与实现注重实用性。1)对于**数据竞争**，ReBranch提供了基于控制流的记录回放来为开发人员重新创建有bug的代码序列，而无需在记录阶段引入高性能开销，也无需在回放阶段引入复杂且耗时的后期计算。2)对于**透明度**，ReBranch提出了低扰动加载，使记录阶段对环境透明，消除了显式加载和其他扰动。3) ReBranch对于分布式系统也采用比例**抽样**，可以有选择地跟踪运行在分布式系统上的执行情况，而不会对系统性能造成很大的影响。

本文的其余部分将继续如下:第二部分简要概述现有研究。在第三节中，我们将介绍我们的观察和想法，以解决如何在多线程程序中有效地重现错误的问题。第四部分介绍了ReBranch的设计与实现，这是一个分布式系统中多线程程序bug回放的实用系统，第五部分对ReBranch的性能进行了评估，并以ReBranch在实际bug调试中的应用为例进行了分析。

### II. R ELATED WORK

后面补

### III. O UR OBSERVATION AND MAIN IDEAS

为了记录和回放多线程程序和分布式系统中的非确定性bug，需要系统地考虑以上所有方面(分布式系统中的数据竞争、透明性和特定特性)。在本节中，我们将展示我们对这些问题的观察，并提出我们的主要观点。

#### A. Data-races

我们已经讨论了现有的研究如何处理II-A中的数据。我们观察到，目前几乎所有的研究都平等地对待控制流和数据流。他们在重放的时候专注于同时重建控制流和数据流。因此，它们要么使用繁重的记录机制来拦截每一次内存访问，要么需要耗费大量时间的事后分析计算。

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh4kv7co2lj30ni0k4ac1.jpg)

然而，通过回顾我们的传统调试经验，我们认识到控制流和数据流信息不是同等重要的。例如，在使用GDB处理图1中的确定性bug时，大多数开发人员在BUG_ON()处首先使用b（设置断点），然后c（继续执行程序，直到下一个断点或者结束），然后等待程序终止;然后bt（）检查调用堆栈。然后开发人员在processing()处调用b，然后是n（单步执行程序，但是遇到函数时会直接跳过函数，不进入函数）和s（单步执行程序，但是遇到函数会进入函数），检查可疑的函数是如何调用helper()的。当开发人员注意到对helper()的额外调用（本不应该发生）时，就会出现根本原因。在整个处理过程中，开发人员不需要查看任何数据流信息(不需要使用x-查看内存地址中的值或p-print命令)。

从上面的例子中，我们注意到两个调试现象:1)开发人员喜欢首先检查控制流;2)对于某些错误，控制流信息足以让开发人员推断出根本原因。尽管可以从调试确定性错误中获得经验，但在对非确定性错误进行记录回放时仍应认真考虑，因为成功的记录回放方法应该提供类似的调试经验。

毫无疑问，理想的记录回放方法应该在回放时重新同时创建控制流和数据流信息。不幸的是，精确的数据流似乎与效率是互斥的:为了精确地重新创建数据流信息，要么需要大量的记录开销，要么需要耗时的事后分析计算。

以前的方法牺牲了同时重新创建控制流和数据流信息的效率。然而，正如前面所讨论的，我们认为数据流信息比效率更容易消耗。

根据我们的观察，我们提出一种基于控制流的方法来记录和重放bug。我们应该注意到，尽管我们关注的是控制流信息，但我们仍然在我们的方法中重播部分数据流信息。例如，保留堆栈信息以支持GDB的bt命令，并且记录来自系统调用和signal的所有非确定性。这是因为数据流信息还为调试提供了一些线索。此外，还有纯粹的数据错误，这些错误只在数据流中表现出来。例如，驻留在科学计算代码中的bug会导致错误结果，标准的调试策略是在每一步观察数据转换，以检测关键转换。然而，在我们关注的程序中，纯数据错误比其他错误更少发生。控制流调试帮助开发人员修复大量的bug;只有在纯粹的数据错误bug时才需要重量级重放记录的机制。

```
Linux 进程间通信 信号（signal）
  1）信号是在软件层次上对中断机制的一种模拟，是一种异步通信方式
  2）信号可以直接进行用户空间进程和内核进程之间的交互，内核进程也可以利用它来通知用户空间进程发生了哪些系统事件。
  3）如果该进程当前并未处于执行态，则该信号就由内核保存起来，直到该进程恢复执行再传递给它；如果一个信号被进程设置为阻塞，则该信号的传递被延迟，直到其阻塞被取消时才被传递给进程。
```

#### B. Transparency

如上文II-B所述，目前很少有研究关注透明度问题。然而，对于当今的大型复杂系统来说，不仅需要重新编译和重新链接，甚至像进程名和进程id更改这样的小干扰也会降低记录重播系统的可用性。我们在下面讨论这个问题。

现代生产系统是由大量组件组成的。为了控制这些系统，需要使用专门的管理员。尽管如此，手工维护复杂的系统仍然是非常具有挑战性的。管理员使用复杂的控制脚本和配置文件来驱动系统和设置可配置选项。随着系统复杂性的增加，控制脚本和配置文件也变得更加复杂。传统的shell脚本和简单的配置文件格式对于现代系统来说是不够的，因为现代系统开始使用高级语言。例如，Tcl已经成为EDA工具自动化[27]的事实标准;Xen[28]使用Python构建控制脚本和配置文件;nginx[29]甚至定义了一种声明性语言来编写配置文件。此外，控制脚本的职责不仅限于初始化和终止进程，还扩展到监视、分析和部分远程控制。

由于这种复杂性，对不负责编写和维护这些脚本和配置文件的开发人员来说，破解这些脚本和配置文件是一项具有挑战性的工作。因此，记录回放系统应该避免给正在运行的环境带来干扰。

根据我们的经验，除了重新编译和重新链接，还应避免以下干扰:

* **改变执行程序的方法program。**启动目标程序的命令行显式或隐式地出现在不同的脚本和配置文件中。变更的需求需要开发人员熟悉许多控制脚本，这也很容易出错。例如，php脚本有时会在其处理过程中调用外部程序，因此改变执行这些程序的方式需要侵入php脚本，而php脚本并不总是由外部程序的开发人员设计的。因此，应该避免显式加载器和类似的机制;

* **更改进程process。**一些系统使用`killall`或类似的命令来根据进程的名称控制正在运行的进程。如果进程名称被更改(如Valgrind所做的)，控制器将无法发送控制信息(如信号signal)到正确的进程;

* **更改进程ID，pid。**有些系统会保存正在运行的进程的PID，并通过它们的PID来控制它们。例如，控制脚本有时使用bash参数 `$!` 检索最近执行的后台进程的pid，然后使用`kill -s`发送信号;CGI进程也通过web服务器的pid进行控制。因此，如果`fork()`、`clone()`或`$!`与实际工作进程的pid(总是由fork自己并跟踪其子进程的加载程序引起)不同，那些控制器机制将会混淆，并向不正确的进程发送控制消息;
* **更改procfs信息。**Linux通过procfs导出许多进程信息，在此基础上构建了许多监视器和控制器。这些信息的任何变化都可能导致这些工具行为不当或报告不正确的信息。

通过分析上述扰动的来源，我们发现，上述扰动大部分是由加载机制引起的:显式加载器或类似的机制改变了程序的启动方式;进程名称更改，因为加载程序接管了加载处理，而不是像往常一样执行execve()系统调用;进程id会改变，因为加载器fork()本身并跟踪其子进程。

因此，为了解决透明度问题，我们提出低扰动荷载原则:

*实际记录播放系统的加载机制应尽可能少地引入扰动。除了避免重新编译和重新链接之外，它还应该消除显式加载器，保存进程名称、进程id和procfs信息。*

#### C. Recording in distributed systems

在II-C中，我们将讨论一些集中于分布式系统的现有方法。它们中的大多数都致力于消息传递范式。但是，对于在生产系统中部署的记录回放系统，还有另一个需要考虑的问题。

与传统的文本日志记录不同，记录回放是重量级的，总是会带来更大的性能损失。

尽管新方法试图减少它们的开销，但对于生产系统来说开销仍然太高。另一方面，这些系统包含许多节点，每个节点都多次执行目标程序，记录如此多的执行会导致大量的数据。相反，开发人员只对少数包含错误的执行感兴趣。

因此，需要采样机制。通过在大型分布式系统中选择一小部分执行进行跟踪，即使记录严重地减慢了所选的执行，对整个系统的记录开销也可以被限制在一个适中的水平上。同时数据容量也降低了。

抽样存在一个内在的问题:没有人能够预测传入执行是否会表现出异常，因此抽样机制有时跟踪正常执行而忽略异常执行运行是不可避免的。然而，根据谷歌在真实的大型系统上的经验，通过调整采样强度，开发人员总是可以得到值得注意的执行模式的痕迹:

*…对于高吞吐量服务，主动抽样不会阻碍最重要的分析。如果一个值得注意的执行模式在这样的系统中出现一次，那么它将出现数千次……低音量服务…能够跟踪每个请求...*

开发人员可以手动选择要跟踪的执行。然而，手工抽样总是需要管理者的工作，这打破了我们在III-B中提出的透明度准则。谷歌的Dapper[30]在跟踪系统中采用自动采样。我们还使用自动抽样来记录重放。

#### D. Our contributions

遵循以上三条原则，我们提出了一个适用于多线程程序和分布式系统的实用记录回放系统ReBranch。与以前的方法不同，ReBranch认为重新创建控制流比重新创建数据竞争更重要。它还具有许多用于在实际系统中记录的重要特性。我们的贡献包括:

* *基于控制流的记录回放*机制，用于多处理器平台上的多线程程序。ReBranch关注于在执行过程中记录控制流。它用牺牲数据流信息换取执行性能和避免事后计算。它完全兼容GDB，并允许用户对确定性程序使用他们的调试经验;
* *低扰动加载*机制，使加载阶段对环境透明。ReBranch避免更改进程名称、PID、procfs信息等。最重要的是，ReBranch不会引入显式加载程序。应用ReBranch只需要一个1字节的二进制补丁。这个特性对于由复杂脚本控制的复杂系统非常重要。

* *比例采样*机制，随机选择可配置的执行比例进行跟踪，同时允许其他执行以全速运行。再分支配合低扰动加载，可以透明地对特定程序的执行情况进行跟踪，而不会对整个系统造成太大的性能损失。

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh4oqgndf8j30um0o6gp1.jpg)

图2显示了ReBranch的三维设计空间。第一个维度是重放精度。重放越精确，它带来的性能开销就越高。ReBranch使用控制流重放。第二个维度是扰动。ReBranch的低扰动负载使它对环境透明。最后一个维度是执行采样。ReBranch引入了比例抽样。

### IV. DESIGN AND IMPLEMENTATION OF REBRANCH

我们在x86 Linux中实现了ReBranch。本节简要介绍其设计和实现。

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh4ovqceouj30zq0kq0ys.jpg)

图3显示了ReBranch的总体架构。它将整个调试周期分为录音阶段和回放阶段。

在记录阶段，通过应用一个字节的补丁，ReBranch允许目标程序像往常一样被加载。加载之后，将生成一个检查点。然后，在执行期间，记录分支指令和系统调用的目标。最后，当程序终止时，将生成一个死检查点来记录最终状态。

当重播阶段开始时，重播程序加载检查点以重新创建进程内存映像。在此之后，开发人员可以使用GDB远程协议来控制分支。在重播期间可以发出所有GDB命令。最有用的命令是b (breakpoint), c (continue), bt (backtrace), n (next line) and s (step into)。回放过程中，ReBranch根据日志log调整分支指令的目标，填充系统调用的结果。ReBranch最终导致开发人员找到bug的位置。

从图3可以看出，ReBranch的主要组件是libinterp.so和replayer。libinterp.so负责加载目标可执行文件的共享对象。我们在IV-A讨论。共享对象还负责记录分支指令和系统调用。在IV-C中讨论了记录阶段。Replayer是在GDB控制下进行回放的工具。它负责加载检查点，然后根据日志重放程序。我们在IV-D中讨论重播。

#### A. Low perturbation loading

ReBranch利用Linux的解释器机制实现了低扰动加载。

解释器是一种用于动态链接的特殊共享对象。在Linux下，每个动态链接的ELF可执行文件(几乎所有可执行文件都是动态链接的ELF可执行文件)都有一个`PT_INTERP` ELF程序头，用于选择其解释器。大多数(如果不是全部的话)ELF可执行文件选择`ld-linux.so.2`作为他们的解释器。选择的解释器与可执行文件本身一起加载。最重要的是，解释器在真正的可执行文件开始之前执行。

ReBranch提供了一个包装器解释器libinterp.so。然后把所有的加载和记录代码都放进去。当libinterp.so被选择为解释器，它在可执行文件被加载之后和执行之前运行。这是一个非常便利的位置，可以让ReBranch发挥它的功能。libinterp.so开始工作，然后加载真正的解释器。在此之后，它激活其二进制检测机制并启动记录阶段。

选择libinterp.so作为解释器，规范的方式是通过`--dynamic-linker`选项重新连接。然而，重新连接的要求违反了我们的低扰动准则。ReBranch提供了一种巧妙的方法来从ld-linux.so.2转换为libinterp.so。图4描述了它。

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh5koehhskj30qu0ledn1.jpg)

ReBranch把libinterp.so复制(或链接)到一个名字和原来解释器只有一个字节区别的文件。因此，通过更改相应程序头的一个字节，开发人员可以轻松地使“跟踪版本”可执行。跟踪版本的可执行文件可以像往常一样执行，不需要修改命令行或特殊的环境变量。

低扰动加载实现了我们在III-B中提出的目标，因为内核根本不知道特殊加载器的存在。当执行“跟踪版本”可执行文件时，内核处理与正常加载相同。因此，几乎所有的流程信息，包括流程名称、pid、procfs条目等，都是保留的。

#### B. Principle of recording and replay

ReBranch通过记录条件和间接分支指令的目标来实现控制流记录。ReBranch还记录了系统调用的非确定性。基本的记录技术是二进制插装:ReBranch用记录指令序列替换所有分支指令，并将所有系统调用重定向到相应的包装函数。此外，在记录过程中，ReBranch会在以下情况下产生检查点:1)当目标程序启动时，2)当创建新的线程或进程时，3)当线程或进程终止时。

检查点是重播的开始点。Replayer负责加载一个检查点来重新创建进程内存映像。之后，replayer启动重播阶段并处理从远程GDB连接发出的传入GDB命令。在回放期间，每次执行命中分支指令或发出系统调用时，replayer将根据分支目标或从日志文件中检索的系统调用结果重新生成效果。



#### C. Recording

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh5koztti9j30ru0uutfq.jpg)

图5演示了在libinterp.so中实现的recorder。recorder的关键技术是二进制插装。通过检测分支指令，程序在分支时将每个分支目标写入日志缓冲区。如果缓冲区已满，ReBranch将缓冲区刷新到日志文件中。为了减少存储开销，ReBranch在使用LZO（数据压缩算法）刷新时压缩日志缓冲区。实验结果表明，LZO可以获得较好的压缩比。

1)*二进制插装*:ReBranch二进制插装类似于Pin[18]。它通过扫描分支指令来动态识别当前的基本块，然后将基本块复制到代码缓存中，并通过记录代码来替换分支指令。代码缓存是由基本块的入口地址索引的哈希表。

出于性能考虑，ReBranch引入了一些优化。首先，与Pin相同，ReBranch将以无条件直接分支结束的基本块直接链接到它的后继块，以避免查询代码缓存。第二，ReBranch对每个被检测的基本块使用分支缓存。当基本块分支到与上次执行相同的目标时，它也不需要查询代码缓存。

2)*多线程支持*:ReBranch引入线程本地存储(TLS)，支持多线程程序。ReBranch的TLS是使用x86 fs寄存器实现的。ReBranch将许多信息放入TLS中，包括日志缓冲区、代码缓存和日志文件名。ReBranch还为TLS区域中的每个线程分配一个2页堆栈。

在发出clone()时创建新的TLS区域。同时，ReBranch为新创建的线程生成一个检查点。之后，新线程将分支和系统调用记录到自己的日志缓冲区中，并刷新到自己的日志文件中。

#### D. Replayer

ReBranch通过打补丁gdbserver实现replayer。Replayer使用gdbserver的原始代码与远程GDB通信，并接管ptrace()控制GDB何时调用继续或单步执行。

图6说明了replayer的基本技术。当它开始执行当前基本块时，它会在该块的末尾设置断点。过一段时间后，程序会碰到断点并暂停。ReBranch然后根据日志扭转程序计数器，然后开始下一个循环。

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh5kpikoe4j31040gg44a.jpg)

#### E. Non-determinism from operating systems

正如我们上面提到的，尽管ReBranch关注于控制流，但它仍然记录了操作系统的非确定性。

ReBranch对于系统调用的记录重放方法是通过给系统调用打补丁`int $0x80`和`call * gs:0x10`，来将所有系统调用重定向到系统调用记录包装器。在回放期间，replayer为每个系统调用调用回放包装器。这些包装器都是特定于系统调用的，它们负责记录和回放来自不同系统调用的不同非确定性。幸运的是，我们不需要为每个系统调用编写两个包装器。首先，大多数系统调用只引入非确定性的返回值。此外，对于许多系统调用，使用宏技巧，记录和回放包装器可以共享相同的代码。图7显示了read()的记录包装器和回放包装器的代码。

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh6xj8mqpfj30zm0c63zr.jpg)

ReBranch也通过安装包装信号处理程序来记录信号处理。当用户定义的信号处理程序将要被触发时，ReBranch首先记录这个事件，然后将控制权转移给用户的处理程序。

#### F. Proportion sampling

加载时采用比例采样机制。在执行其他操作之前，libinterp.so先检查环境变量是否有用户定义的比例。然后使用rdtsc生成一个伪随机数。ReBranch根据伪随机数与用户定义比例的比较结果决定是否跟踪后续执行。如果未选中，则使用libinterp。这样就可以在不进行任何处理的情况下立即将控制权转移到原始解释器。然后，未选中的程序以未跟踪的方式运行。

与LD_PRELOAD不同，上面提到的环境变量是私有的:它不影响任何其他程序。因此，它可以全局设置。

### V. E VALUATION

本节评估ReBranch的性能，然后分享我们的调试经验。

#### A. Pure performance penalty

我们首先评估ReBranch对单个进程的纯性能损失。有四个真正的程序被选择:lame [31]， wget [32]， lighttpd[33]和memcached[34]。结果列在表I. Lame是一个mp3编码器，这是一个典型的CPU密集型程序，我们测试了它编码一个7分钟的波格式音频文件到mp3格式，下面列出的是总编码时间。我们通过从局域网下载一个大的零填充文件来测试wget，下面列出的结果来自于下载后自己报告的速度。Lighttpd测试由apache ab[35]驱动:ab us用于访问由Lighttpd提供的一个小的静态web页面，并列出ab报告的吞吐量，我们将ab的并发性设置为20。对于memcached，我们使用antirez的mc-benchmark[36]。memcached和mc-benchmark都使用默认设置。

所有测试都在x86 PC上完成，CPU为2.2GHz Intel Core 2 Duo E4500，内存为2GB。软件环境是最新的Gentoo Linux内核2.6.34。所有应用程序都使用低扰动加载。这里列出的所有测试结果都是三个测试的平均值。

表I显示了跟踪不同类型的应用程序时ReBranch的纯性能损失。与基于RW草图的PinPlay[16]和PRES[15]等二进制检测方法相比，ReBranch的结果要好得多。PinPlay将单线程CPU密集型基准测试降低了100倍，性能损失随着线程数量的增加而增加。PRES也减慢目标程序从70%到300次，当记录在RW草图方案。

结果我们发现，由于网络瓶颈，在正常执行中，程序在总执行时间中有90.4%被调度出来等待传入数据。当再分支追踪时，等待时间降至50.4%。lighttpd和memcached测试的性能损失介于lame测试和wget测试之间。表I还显示了日志压缩的有效性。

日志数据率似乎很高(远低于RW方案下的PRES[15])。但是，日志刷新是在后台完成的，并且会重叠计算，所以不会花费太多。此外，我们还可以使用定期检查点[8]、[14]技术来消除未使用的日志。

#### B. Proportion sampling

我们使用一个模拟的分布式web服务器集群来评估比例抽样。我们运行了100个lighttpd实例，每个实例提供一个小的静态web页面。在这些lighttpd实例的前端，一个nginx[29]服务器充当一个负载均衡器。它将所有http请求重定向到上游worker循环。结果列于表二。0%的结果证实，当不被跟踪时，工人可以全速运行。随着跟踪比例的增加，我们还可以看到性能的下降。此外，我们可以看到，在跟踪相对较小的执行比例时，性能损失要比跟踪所有执行时低得多。

#### C. Real debugging experiences using ReBranch

我们在自己的系统中使用了ReBranch，并取得了一些经验。在这里，我们将分享我们的调试经验和一个修复memcached中的bug的案例研究。

图8显示了使用ReBranch的常见调试处理。首先，开发人员应该注意到bug的存在，并识别出有问题的程序。在开发人员决定跟踪哪个程序之后，他们可以对它应用一个字节的补丁(使用xxd和xxd -r)，并在全局环境设置(bash概要文件)中设置采样比例。然后，开发人员必须等待bug被跟踪。在得到错误跟踪之后，开发人员可以开始循环调试。在调试期间，有时开发人员需要bug跟踪和正常跟踪来比较控制流的差异。最后，开发人员可以隔离和修复错误。在解打补丁程序(也一个字节修改)，它可以正常运行。

memcached是一个高性能的分布式内存对象缓存系统[34]，它部署在许多商业系统中，如Wikipedia、YouTube和Twitter。memcached是一个多线程程序。

memcached bug 106[37]的症状是死锁:当使用二进制协议和UDP连接时，如果数据包的大小大于他们声称的大小，一段时间后系统将锁定，CPU利用率提高到100%。这个错误在2009年11月12日被报告，直到2010年8月才得到修复。

我们采用ReBranch来帮助我们调试，最终找到根本原因。这个问题实际上是由两个独立的bug引起的。

死锁是由错误处理代码中的错误引起的。下面的代码就是原则。

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh5klde0jaj30zo0cwq3s.jpg)

当处理来自UDP套接字的请求时出现错误时，memcached只清理相应的内存并将套接字置于“关闭”状态。如果UDP套接字包含未使用的数据，前端epoll_wait()调用总是立即返回，触发状态机处理事件。然而，处理“关闭”套接字的代码忘记了排干这些数据，使程序陷入死锁。

死锁是一个控制流问题，适合重新分支。当我们处理这个问题时，我们首先在recvfrom()处设置断点并计算它处理了多少包，然后使用GDB步进调试重新播放，以检查在最后一次recvfrom()之后发生了什么，最后我们确定了未终止循环。在那之后，根本原因就很容易推断出来了。

当从UDP套接字读取时，程序通过将传入的数据放到接收缓冲区的头部(上面代码的第2行)来删除前一个请求的未使用数据(正常情况下请求不应该留下未使用数据)。然而，代码错误地计算了缓冲区的大小(上面代码的第5行)。因此，主状态机错误地认为存在未完成的请求。偏差达到24(完整头部的大小)后，程序试图解析假的“请求”并得到一个错误。

处理第二个bug稍微困难一些，因为没有显式的控制流异常。我们必须比较正常执行和有缺陷执行之间的执行流，并发现，在有缺陷执行中，状态转换顺序与正常执行中的顺序不同:它在完成一个请求后错误地进入到某个状态。作为线索，我们仔细检查了相应if语句的条件，最终找到了根本原因。

我们的经验显示了控制流重放的威力，它提供的信息比传统的日志挖掘多得多。控制流重播提供了与循环调试类似的调试体验。它允许开发人员使用他们熟悉的工具来控制执行。例如，设置断点、步进下一行、步进函数、监视调用堆栈等等。相比之下，传统的日志挖掘只允许开发人员执行头脑中的程序。

### VI. C ONCLUSIONS

本文讨论了在多线程程序和分布式系统中利用记录回放再现非确定性错误的问题。我们提出了3种机制，这对实际的录音重放方法很重要。首先，我们对调试过程有了一个新的认识，并提出了可以放松同时再现控制流和数据流信息的要求。因此，我们提供了控制流记录回放机制，允许开发人员使用GDB检查错误的代码序列，而不会在记录阶段引入沉重的开销。其次，我们最初强调了记录过程中的低扰动要求，这一直被忽略，并提出了低扰动加载机制。在大型和复杂系统中，这种机制使得记录阶段对环境和管理员透明，因为开发人员只能通过一个字节的补丁启用跟踪。第三，我们采用比例抽样来增加灵活性，从而可以根据需要控制整个系统的性能损失。

我们在ReBranch——一个为多处理器平台和分布式系统上的多线程程序设计的实用录重放系统中实现了上述机制，并在http://gitorious.org/rebranch公开发布。评估表明，在对环境的低干扰的情况下，ReBranch比以前的方法执行得更好，同时为用户提供良好的调试体验。此外，我们已经使用ReBranch成功地修复了实际系统中的一些bug。