---
layout:     post
title:      InSight A Framework for Application Diagnosis using Virtual Machine Record and Replay
subtitle:   ()
date:       2020-07-18
author:     qzwerty
header-img: img/blog-bj-1.png
catalog: true
tags:
    - RR security


---

## InSight A Framework for Application Diagnosis using Virtual Machine Record and Replay

非确定性执行对软件系统(用户级应用程序和操作系统)的诊断(调试、分析和执行状态挖掘)提出了几个挑战。虽然存在使用修改过的库、库包装器、二进制工具和内存隐藏技术的几种技术，但我们的目标是利用虚拟化所支持的记录和回放技术来提供通用诊断框架。我们的解决方案的动机是执行的可重现性需求、不修改应用程序源代码、没有或最小的二进制插装开销——所有这些都是现有技术很少提供的。我们的诊断框架InSight由两个阶段组成——第一个阶段记录应用程序和虚拟机的执行状态，第二个阶段回放和分析记录的执行。我们在基于Linux内核的虚拟机(KVM)平台上实现了InSight，作为贡献，我们实现了KVM的高效记录和重放基板以及使用该基板的诊断框架。本文介绍了这些组件的设计和实现，并开发了一套诊断工具——潜在死锁检测、锁使用情况分析和功能分析。我们也提出实验来证明正确性，洞察力的低开销和相关的诊断结果。

**Keywords**

Virtual machine record and replay, application diagnosis

### 1 INTRODUCTION

诊断是软件系统(用户空间应用程序和操作系统)正确和有效工作的一项重要活动。不同变量的诊断包括调试、分析和执行状态挖掘，通常用于此目的。诊断服务有不同程度的好处(就诊断所需的信息而言)，也有不同程度的费用。

像gdb[1]和gprof[2]这样的工具需要使用调试信息选项来编译二进制文件以便获取状态单步执行。其他工具，如ftrace[3]、systemtap[4]和PinPlay[5]，需要启用脚本来提取所需事件的信息。像valgrind[6]这样的诊断工具使用重量级的插装来提取信息，并且由于在线诊断而导致较高的开销。此外，像OProfile[7]这样的分析工具利用硬件计数器对运行时执行状态进行采样，并且需要与感兴趣的应用程序一起执行。每个工具都提供了诊断功能的子集，并且需要不同程度的执行开销和用户干预(在设置诊断环境方面)。诊断越“精细”，成本(就执行时间而言)就越高。例如，考虑一个写保护内存区域，用于跟踪进程/线程的所有写操作。由于每次内存写入都被捕获，应用程序很少能以本机速度执行。对于粗糙的诊断设置，不能检索所有所需的信息，例如，分析工具只提供执行状态、调用图等的定量分析。另一个诊断需求是跟踪多个实体(进程/线程)，以捕获它们之间交互的时间顺序，例如，进程间调用等。这些需求通常通过使用所有相关实体的细粒度捕获执行来满足，这可能会增加执行开销。

此外，当问题的来源未知时，通常需要诊断工具，例如，bug是不确定的，死锁依赖于线程的交叉离开顺序等。使用分析和调试器的运行时诊断无法捕获未发生的条件。一个有用的需求是能够记录系统的执行状态，并定期或在感兴趣的事件发生时，使用各种诊断工具来处理它，以发现所需的信息。

作为这项工作的一部分，我们的目标是提供一个诊断应用程序的框架，其目标如下:不确定性执行状态的再现性、最小化执行开销和检测需求。我们的诊断服务框架InSight利用了虚拟化环境中可用的记录和回放功能。该框架为诊断提供了通用且快速可配置的服务，同时最大限度地减少了对应用程序执行的干扰。虚拟机记录功能允许以确定的方式重播对虚拟机(客户操作系统和应用程序)的非确定性状态的记录。对于不同类型的诊断需求，可以多次重播执行。在Crosscut[8]中使用了类似的方法，其中多阶段重放用于提取集中时间间隔和选定进程的执行状态。提取的状态可以在诊断工具valgrind或perl执行环境中重播。

作为诊断框架的一部分，我们使来宾操作系统与主机(hypervisor)协调，以交换关于事件的状态信息。修改后的guest的一个优点是，要诊断的应用程序不需要担心与状态收集相关的基本检测，只需少量配置就可以快速使用诊断服务。虽然客户检测需要提供事件状态信息(例如，进程映射的指令指针、符号表等)，但事件本身的生成依赖于现有的操作系统工具。

提供此服务的一个重要要求是最小化干扰——低开销的状态收集和可靠地重现执行状态以进行诊断。我们利用带有插装的记录和回放功能来满足上述需求，并通过InSight，我们的诊断即服务框架，做出以下贡献:

* 为基于linux的内核虚拟机(KVM)平台设计并实现一个优化的记录和回放基板。我们的基板支持网络流量重放和始终基于dma的磁盘访问。

* 测量记录和重放系统以获得可重现的诊断状态。

* 实证验证诊断框架的正确性和开销。

* 提供一组诊断工具、锁争用分析、潜在死锁检测和函数级分析，以演示InSight的适用性和可用性。

### 2 THE INSIGHT ARCHITECTURE AND IMPLEMENTATION

InSight由各种组件组成，以实现可重现性和可忽略的开销/失真的执行跟踪收集。在本节中，我们将描述InSight的各个组件以及这些组件如何协同工作。本节还描述了InSight组件的实现。

#### 2.1 架构

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh12e0myudj30s80e8ju3.jpg)

如图1所示，InSight由两个主要组件组成:(i)支持记录和回放的hypervisor，和(ii)诊断子系统。与hypervisor协作，虚拟机充当执行应用程序的平台，并便于提取执行事件跟踪。诊断子系统使用存储在事件数据库中的事件跟踪来回答有关执行的有趣查询。

在InSight框架中，来宾vm以记录模式(应用程序执行阶段)执行。在这一阶段，不收集诊断信息，只记录关于外部(非确定性)事件的信息，这是重放所必需的。在稍后的时间点，可以重新播放虚拟机(及其应用程序)的记录状态，以进行诊断——诊断阶段。在回放期间，所有外部事件都被正确地注入，以使应用程序和操作系统执行相同的执行。此外，事件探测将事件的发生通知InSight系统。我们实现的事件探测集将在第3节中描述。

InSight框架使用一个仪表化的客户虚拟机，即，客户虚拟机存储有关事件的状态信息并向管理程序公开状态信息。例如，在一个锁获取事件上，虚拟机会显示获得锁的线程。由于InSight和probes都知道用于信息交换的预定义结构，所以InSight收集事件probes准备的信息，并将它们存储在events DB中用于诊断。

所有状态信息都存储在特定于事件的预定义结构中，供hypervisor使用。在诊断期间，hypervisor逐步捕获所有感兴趣的事件的状态信息，并将它们存储在事件数据库中。诊断工具进一步使用事件数据库来分析应用程序执行。

InSight架构允许并行执行多个诊断阶段，从而能够对长时间的测试场景进行有效的分析。例如，数据竞争分析可以与潜在的死锁分析并行进行。

#### 2.2 Record and replay implementation

InSight的两个主要组件中的第一个是虚拟机记录和重放基板。记录和回放特性使虚拟机的执行具有再现性。InSight的虚拟机记录和回放旨在实现忠实的回放和最小化日志开销。记录和回放系统遵循非确定性外部事件记录的标准实现技术[9,10]。作为InSight的一部分，我们设计并实现了网络事件的记录和回放以及基于dma的磁盘IO。根据我们的文献调查，我们还没有遇到同时考虑这两个因素的记录文献和重放文献。

基于dma的IO和网络事件的高效记录和重放实现对于IO绑定的应用程序(磁盘和网络)的可靠执行非常重要。

在Linux内核虚拟机(KVM)上实现了InSight记录和回放。基片记录非确定性的外部事件，如从外部设备读取的数据(键盘、鼠标、磁盘)，注入到虚拟机的外部中断，以及由外部设备复制到VM内存中的数据(DMA读取、网络包接收)。重播是通过在记录阶段事件发生的同一实例中将这些事件重新注入客户虚拟机来实现的。作为事件注入引用的时间戳与外部事件一起记录。我们使用<分支计数器、指令指针、ECX值>元组作为时间戳，它标识在[9]中讨论的执行中的任何点。

接下来，我们将简要描述我们对记录和回放技术的主要贡献(作为InSight的一部分)。

##### 2.2.1 Deterministic replay of network events

为了使具有网络连接的虚拟机能够独立记录和重放，我们记录了传入数据包的状态。（独立指重放时不需要有网络连接）重放网络数据包到达的挑战是来宾使用的数据包是不确定的。在轮询模式下操作的客户设备驱动程序可以在任何时间使用接收到的包。用一个完整位表示是否在接收缓冲区中接收到包(当前)。

为了记录包的状态，我们修改了QEMU[11]中的Intel eepro100网络设备仿真。仿真是基于[12]中提供的设备规范实现的。

在回放期间，为了确保与记录期间同时读取数据包，我们将(接收到的数据包的)完整位的设置与定义良好的来宾事件关联起来。例如，QEMU会延迟设置接收到的包的完整位，直到引发一个带有时间戳的事件(中断)或来宾VM事件(基于端口/mmio的IO事件)到达后再设置。如果在定义良好的事件发生时设置完整位，而不是在接收包时设置完整位，那么我们可以确保来宾读取接收到的包与记录的事件相关联。

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh134uo99qj30sg0iu76o.jpg)

图2中的示例显示了在客户执行期间数据包pkt 1、pkt 2、pkt 3和pkt 4的到达。但是这些数据包的完整位直到一个定义良好的事件e才被设置。这种技术在定义良好的事件和接收到的数据包集之间创建一个happened -before关系。在我们的例子中，来宾VM只能在它接收到事件E之后才能看到包(因此这些包是间接绑定到事件E的)。在记录过程，每当一个定义良好的事件到达时，会将设备接收到的未设置完整位的包记录在一个日志文件中，并设置相应的完整位。在图2中，缓冲包1-4在事件E期间被记录，并且E的日志记录将有缓冲包的列表pkt 1..4。在重播期间，在注入一个定义良好的事件之前，我们检查是否有任何随事件记录的包，如果有，我们复制这些包接收缓冲区和设置他们相应的完整位，以此来传递这些包。在图2中，当来宾执行到达重播时间i时，hypervisor应该注入事件E。在注入事件E之前，hypervisor找到pkt 1..4用事件E记录并复制记录的包内容放进接收缓冲区，并在恢复客户执行之前设置完整位。保存/恢复包内容、设置完整位和记录/注入定义良好的事件这三种操作都是来自客户机的透视图的原子操作——客户机看不到中间状态。

##### 2.2.2 Replay of Direct Memory Access(DMA)-based disk I/O

实现磁盘访问的有效记录和回放有两个主要挑战，(i)为回放维护一致的磁盘状态，和(ii)支持不同的访问方法，基于端口或基于dma的IO。为了解决第一个问题，我们使用QEMU提供的基于写时复制(COW)的磁盘格式。使用写时复制磁盘映像进行记录，客户机生成的写操作被重定向到临时磁盘文件(保留初始磁盘映像的状态)。为了重放，我们使用相同的“写即拷”磁盘映像，该映像具有磁盘的初始状态，并再次将写操作重定向到一个临时磁盘文件。在记录和重播中，从临时磁盘文件提供写后读取。该方法允许初始磁盘状态保持不变(在记录和回放期间)，并且可以用于任意数量的回放执行。

为了执行IO密集型应用程序，我们提供基于dma的磁盘访问(而不是基于端口的IO)，并记录和回放相同的数据。我们最初的实验表明，基于端口的IO是相当慢的比较基于DMA的IO，不能处理甚至中等IO密集的应用。基于dma的磁盘访问是异步的。即，磁盘访问由来宾启动，并通过磁盘驱动器的中断通知完成。在记录和回放场景中，DMA完成时间——从DMA开始到DMA完成中断到达的持续时间——在记录和回放阶段应该是相同的。如果没有保留基于dma的IO的完成时间，重播将是不忠实的。

在回放期间有两种可能的场景，基于dma的磁盘访问在回放期间比记录期间完成得更早，反之亦然。为了在重播期间保持相同的事件顺序，我们延迟guest的执行直到相应的DMA中断到达。这两种场景如图3所示，

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1ghdnr8b4fyj30t60jydip.jpg)

1. 在记录期间在时间i完成的DMA，在重播期间在更早时间i−∆完成。在这种情况下，DMA完成中断被延迟到重放来宾执行到达时间i。

2. DMA在记录期间在时间j完成，但是当来宾执行在重播期间到达时间j时，DMA还没有完成。我们暂停客户执行，直到DMA完成(时间j +∆)。

InSight依赖于DMA中断计数器来实现这一点。计数器表示到目前为止收到的DMA中断的数量。我们维护两个计数器，记录计数ADMA计数和实际计数ADMA计数。实际计数(VM状态变量)在模拟磁盘驱动器引发DMA中断时增加。记录的计数是日志记录中的一个字段。在记录阶段，每次一个DMA中断被记录，为那个DMA中断记录记录的计数被设置为实际计数的当前值。在重放期间，在从日志文件中注入DMA中断之前，实际计数和相应的(中断记录的)记录计数值被比较。如果实际的计数少于记录的计数，那么DMA还没有完成。在这个场景中，客户机VM进程被主机添加到一个可中断的事件等待队列中。当计数器匹配时，客户进程将从等待队列中删除并继续执行。

另一种可能的情况是实际的ADMA计数大于记录的RDMA计数不会发生，因为重播保持了确定性——没有额外的DMA可以被启动。

3. THE INSIGHT DIAGNOSIS FRAMEWORK

在InSight框架中，应用程序诊断有两个步骤。在本节(和下一节)中，我们将介绍设计和实现细节以及InSight的一组用例，即诊断即服务框架。框架本身是通用的，可以应用于几个诊断问题。通过诊断，我们意味着在应用程序执行期间收集状态信息，用于调试、分析和执行分析。

3.1 InSight events

InSight的诊断工具脱机工作，并依赖事件跟踪来分析应用程序。事件跟踪是通过收集有关应用程序执行的关键事件生成的。根据要收集的信息是否需要对事件生成和事件信息进行检测，对事件进行分类。

显式事件需要事件探测，这是一个帮助通知事件发生和事件信息提取的代码片段。例如，锁获取、锁释放等事件需要特殊的工具从应用程序的上下文中生成事件。事件探测是应用程序执行的一部分。另一方面，可以提取隐式事件，而不需要对应用程序进行任何插装和执行。虚拟化层(软件和硬件)促进了隐式事件的提取。例如，可以使用虚拟化引入的额外页表级别来提取特定内存地址上发生的内存访问事件。每个事件都有两个重要的需求，事件通知的检测和从事件中提取的信息。例如，提取锁事件涉及在锁事件期间获取通知，并提取与锁的地址和获取锁的线程的身份相关的信息。

3.1.1 Event probes

显式事件是那些捕获应用程序执行上下文中的特定事件的事件。在应用程序执行路径中插入事件探测以生成显式事件。

我们的第一个事件探测方法基于大头针工具[13]。我们使用Pin API在应用程序的可执行文件中插入必要的事件探测。Pin不需要修改源代码，但是它使用JIT重新编译并在虚拟环境中运行应用程序。现有工作[14]报告在Pin虚拟环境中运行应用程序的平均开销为20-30%(没有事件探测和相关操作)。

插入事件探测的第二种方法适用于从标准库函数生成事件并在共享库中定义的情况。基本思想是在所需的库函数周围创建包装器。包装器是使用ld_ PRELOAD环境变量创建的。此变量用于指定在执行应用程序时要加载的库。在应用程序的地址空间中加载任何其他共享库之前，先加载LD _ PRELOAD库。按照惯例，动态链接器通过在所有加载的库中按顺序搜索来链接一个库函数调用，然后链接找到的函数定义的第一个实例。库函数的包装器可以通过使用链接实现。要理解这一点，请考虑图3.1.1中的场景。在没有LD _ PRELOAD包装器的情况下，evince应用程序中的pthread_mutex_lock()和malloc()调用被链接到libpthread-2.15。所以,libc - 2.15。所以分别。使用LD _ PRELOAD包装器，这些调用被链接到在libeventprobes中定义的包装器函数。所以图书馆。随后(在记录所需的事件信息之后)，包装器函数将控制切换到预期的函数调用。使用动态加载器ld的API的dlsym()函数获得实际函数定义的地址——给定函数名将返回函数定义地址。

InSight的初始usecases使用基于LD的预加载方法来插入事件探测，但我们的框架足够通用，可以容纳其他方法来捕获用户级函数(尽管在更大的插装和运行时开销下)。我们当前的实现使用修改过的客户内核来处理与操作系统相关的事件。systemtap[4]还可以用于插入内核级事件探针。systemtap允许通过guru模式2嵌入C代码，它可以用来生成事件。

3.1.2 Event information and event buffer

通过在来宾应用程序的地址空间中使用event_buffer，事件的信息从事件探针交换到诊断系统。事件探测在事件发生时准备所需的事件信息。通过准备有关事件的基本信息，开销得到了控制。例如，锁事件由锁地址、任务的pid、lwpid(线程id)和锁请求的时间戳组成。可以提取这些细节，而不会带来很高的开销。为了进一步减少开销，将经常使用的和不变的信息(pid、lwpid、进程名)缓存在内存中。

在将事件信息提供给诊断系统方面还有其他一些微妙的挑战。事件信息在应用程序的地址空间中，使用来宾虚拟地址(gva)进行寻址。而访问事件信息的诊断系统在主机地址空间中执行，并使用主机虚拟地址(hva)。此外，如果event_buffer(用于事件信息的内存)跨越多个页面，我们需要进行不止一次从gva到hva的地址转换。为了确保总是一次转换就足够了，我们在客户机中使用了页面对齐的event_buffer(其大小小于一个页面)。在多线程应用程序中，事件可能发生在不同线程的上下文中。使用全局事件缓冲区，线程可以覆盖另一个线程准备的现有事件信息。为了避免这种情况，我们使用特定于线程的event_buffers来获取事件信息。

3.1.3 Event notiﬁcations

诊断系统通过接收来自事件探针的事件通知来识别事件发生情况。要生成洞察事件，事件探针执行以下操作:

1. 在event_buffer中准备有关事件的必要信息。这些信息由通用字段和事件特定字段组成。

2. 将event_buffer的来宾虚拟地址(gva)复制到EAX寄存器。

 3.将洞察事件通知键E VENT N TIFY存储在EBX寄存器中。

4. 使用int3引发调试异常。

前两个步骤准备事件信息，最后两个步骤生成通知。使用一个调试异常来通知事件，该调试异常由hypervisor捕获并由诊断系统处理。为了区分事件通知异常和其他异常，EBX寄存器中存储了一个特殊的值E VENT N OTIFY。Hypervisor在应用程序执行阶段和诊断阶段以不同的方式处理事件通知。

3.2 Stage 1: Application execution

应用程序执行是InSight框架的第一个阶段。应用程序在启用记录和回放的虚拟机中执行。虚拟机监控程序被配置为以记录模式操作虚拟机，该模式允许记录所有感兴趣的事件。

应用程序执行在插入必要的事件探测(操作系统级和用户级探测)之后开始。图5显示了VM记录过程以及事件通知。在应用程序执行(VM记录)阶段，事件信息和事件通知是不必要的，通过避免引发这些异常，应用程序执行可以避免这种开销。为了避免捕获事件通知，我们可以使用硬件支持来禁用捕获所有调试异常，或者使用软件插装来禁用仅捕获为事件通知而引发的调试异常。我们选择后面的方法，这样就不会改变正常的调试异常处理。图5显示了调试异常导致VM E退出时的方法。通过用nop指令替换事件探测使用的int3指令，该方法避免了捕获事件通知。替换的第一步是识别导致调试异常的int3指令的地址。我们允许第一次出现调试异常。每次发生调试异常时，如果虚拟机处于记录模式(应用程序执行阶段)，并且EBX寄存器已经注册成功，我们将当前指令指针指向的指令替换为nop指令。

3.3 Stage 2: Application diagnosis

诊断阶段从VM重放开始，重放以前记录的应用程序执行。VM的确定性重放特性确保事件探测器将准备相同的事件信息，并将像记录阶段一样发出相同的显式事件通知集。此外，在VM回放记录的执行时，诊断阶段可以生成隐式事件。与忽略事件信息和事件通知的应用程序执行阶段不同，诊断阶段记录所有事件信息。应用程序执行阶段和应用程序诊断阶段之间的唯一区别在于事件通知方法。在诊断阶段，int3指令不会被nop指令取代。在重播过程中，每次出现调试异常时，诊断系统处理如下:

* 1.检查EBX寄存器中的洞察事件识别键。

* 2.如果EBX寄存器中不存在EVENTNOTIFY，按照默认的调试异常处理。

* 3.否则，将EAX中存在的event_buffer的来宾虚拟地址(gva)转换为主机虚拟地址。
* 4.从转换后的hva地址读取事件信息，并将该信息存储在事件数据库中。

诊断系统收集并保存事件数据库中的信息，诊断工具可以使用存储的事件进行分析。我们当前的实现使用平面文件作为事件数据库。我们在函数分析器的实现部分解释了隐式事件的提取(见4.4节)。

### 4. USECASES

在本节中，我们将解释为演示我们的框架的有用性而创建的原型工具。

4.1 Events used by diagnosis tools

为了演示InSight的有用性，我们实现了一些工具，它们可以分析与锁使用相关的问题，并对应用程序执行进行概要分析。这里我们讨论为实现这些工具而生成的事件集的信息字段。每个事件类型都包含特定于该事件类型的信息。除了特定于事件的字段外，还会提取每个事件的公共字段，如pid、lwpid、进程名和时间戳。

LOCK, T RY OCK, T IMED OCK, U NLOCK, LOCK I NIT事件是由相应的pthread_mutex_*包装器函数生成的，锁地址表示事件特定信息。类似地，{L OCK, T RY OCK, T IMED LOCK, U NLOCK}R ET事件是在实际定义返回后由包装器函数生成的。在这种情况下，返回值和锁地址是特定于事件的信息字段。T HREAD C REATE和F ORK事件分别由pthread_create和fork函数包装器生成。在执行start例程之前，从新创建的线程生成HREAD C REATE。事件是从创建的子进程中生成的。这两个事件都包含父节点的lwpid信息。C ONTEXT S女巫是由内核生成的，包括从和到进程的lwpid。E XIT在exit、_Exit、_Exit调用期间生成，或者在从应用程序地址空间卸载库时从包装器库析构器调用生成。在前一种情况下，E XIT包含退出函数的返回值，在后一种情况下，返回值为0。S的YMBOL事件在每次执行时生成一次，并且在第一个显式事件之前完成。本例中的事件信息包括symbol_buffer的地址(参见4.1.1节)，AMPLE(隐式)事件是在应用程序执行的每个采样周期sample_interval中生成的(参见4.4节)，并包含指令指针值作为事件信息。

##### 4.1.1 Symbol table extraction

为了有效诊断，诊断系统产生的结果应包含被诊断应用程序源代码中的符号。这有助于用户将应用程序执行中发现的问题映射到应用程序源代码。为了实现这一点，诊断系统应该能够映射堆栈中的原始地址和指令指针到符号。在生成事件时，将地址映射到事件探测中的符号会给应用程序的执行带来很高的开销。我们将这种测绘推迟到诊断阶段。在应用程序执行阶段，我们生成一个公开应用程序符号表的S YMBOL事件。该事件由一个4KB对齐的页面边界符号s_buffer组成。符号表通过迭代地引发S个YMBOL事件公开，而每个事件通过将符号表复制到symbols_buffer公开它的一部分。每当符号s_buffer满了，就会引发一个S的YMBOL事件。与其他显式事件一样，仅在诊断阶段提取并使用symbols_buffer中的信息。

#### 4.2 Lock contention analysis

锁争用是多线程应用程序中一个重要的性能指标。由于锁争用，设计糟糕的线程协调可能会导致并发性的巨大松散。因此，对于不同的工作负载特征，分析应用程序中出现的锁争用级别非常有用。我们实现了一个锁争用分析器，它使用InSight框架生成的事件跟踪来分析多线程应用程序中的锁使用情况。

该工具的锁争用输出受到了mutrace工具[15]的启发，该工具为pthread锁提供了在线轻量级锁争用分析。我们的锁争用分析器对生成的事件跟踪进行操作，而不是在应用程序内部运行(与mutrace一样)。这样做的好处是减少了开销，同样的锁事件可以用于分析其他问题，比如潜在的死锁。

清除获得锁的线程。我们使用一种间接的方法来识别被阻塞的锁请求。例如,考虑前三个事件的事件跟踪(L下L、T 1), (L下L、T 2)和(L下L、T 3), L是一个锁,我是线程试图获取锁L中这三个锁请求执行,只有其中一个不会阻止(假设锁L可用这些事件之前)。但是在这三个线程中，获得锁的线程T j只有在(L OCK _R ET L, T j)事件发生时才知道。为了计算线程T j获得的锁L上阻塞的线程数，我们维护了一个已发出锁请求但尚未获得锁的线程数。在L OCK _R ET事件中，这个计数用于估计锁上被阻塞的线程的数量。

#### 4.3 Potential-deadlock detection

在多线程应用程序中，由于线程交错的无限可能性，很难识别所有潜在的错误。这些难以捕获的错误是由特定的线程交错引起的，而这在测试阶段一中可能不会发生，这样的错误是死锁。我们实现了一个潜在死锁检测工具，它使用InSight收集的事件跟踪，并使用现有的潜在死锁算法[16]。

潜在死锁检测包括三个主要步骤，

(i)构建锁树，(ii)添加交叉边，和

(iii)运行改进的深度优先搜索(DFS)进行周期检测。

为每个线程构建锁树。锁的线程T树,由几种路径从根R每个节点N每条路径从根树中的其他节点代表一个堆栈栈的底部(根节点)持有的锁的线程的执行。可以使用InSight框架生成的锁和解锁事件轻松构建锁树。换句话说，锁树表示持有-等待死锁条件下所有可能的持有场景。在构建锁树之后，为了表示线程之间潜在的等待关系，我们在锁树之间添加交叉边。当且仅当节点N和M表示同一锁，属于不同的锁树时，才添加N和M之间的交叉边。在图6中,我们使用的符号我T来表示一个锁被线程T和后缀我添加识别各种实例锁锁定的线程X树在图6中,节点1 X和Y节点1代表分别锁定和收购X和Y,所以它们之间的交叉边缘添加(注意后缀不重要)。

##### 4.3.1 InSight’s potential-deadlock detector

如果锁不可用，trylock不会阻塞，但是线程可以阻塞使用trylock获得的锁。再次参考图6，边缘b2 X→b1 Y被删除，因为节点b2 X表示X使用trylock获得的一个锁，这个锁在任何线程上都不会阻塞。另一方面，一旦X使用trylock获得锁B，线程Y就会阻塞锁B在X上，因此边缘b1 Y→b2 X是有效的。我们的洞察潜在死锁检测器认识到这种情况，并构建锁树，其中单向交叉边表示基于trylock的锁获取。此外，pthread的timedlock被认为是一个trylock。对于trylock和timedlock，只有当它们获得锁时，锁节点才会被添加到锁树中。

我们的技术使用LOCK, T RY LOCK, T IMED LOCK, LOCK R ET, T RY OCK R ET, T IMED LOCK, U NLOCK, U NLOCK R ET事件来构建锁树(交叉边)。接下来，我们使用基于dfs的算法[16]来检测有效的锁周期。

#### 4.4 Function proﬁler

函数分析器提供函数级统计信息，比如每个函数花费的执行时间百分比，并帮助识别应用程序中的热点。我们使用InSight框架的隐式事件集合和地址空间识别特性构建了一个函数分析器。如前所述，隐式事件是从来宾应用程序透明地收集的。为了支持隐式事件，主机需要知道为哪个应用程序收集了隐式事件、地址空间信息(将来宾虚拟地址转换为主机虚拟地址)和解析地址的符号表详细信息。本节详细说明隐式事件集合及其在函数分析中的使用。

##### 4.4.1 Application identiﬁcation

作为InSight的一部分，主机维护被跟踪的任务列表——需要分析信息的任务(tracked_lwpids)和在感兴趣的vcpu上执行的当前任务(current_lwpid)。tracked_lwpid中的每个lwpid对象由lwpid、pid、counts_to_sample(用于采样)组成。任务是客户机中的进程或轻量线程(pthread)。这些变量在创建新任务、跟踪任务退出以及上下文切换期间进行更新。为了识别新任务的创建，我们使用已经跟踪的任务生成的事件。注意，未被跟踪的任务将不会生成这些事件。在运行时，主机将current_lwpid更新为当前事件的lwpid，并将lwpid添加到tracked_lwpids。这是因为FORK和THREADCREATE事件是在执行新创建的任务时生成的。任务退出由E退出事件标识，主机在此事件上删除tracket_lwpid中的所有任务，将pid作为E退出事件的pid。注意，退欧事件仅由客户内部的进程生成，而不是由线程生成。主机还将current_lwpid设置为N ULL。在从任务P到任务Q的C ONTEXT S切换事件期间，主机更新current_lwpid。如果Q位于tracked_lwpids中，则主机将current_lwpid设置为Q。否则，current_lwpid将设置为NULL。to的主机现在可以使用current_lwpid随时知道哪个进程在vcpu上运行(以关联收集到的隐式事件)。

##### 4.4.2 Sampling application execution

我们定期对应用程序的执行进行采样，并在每个采样中收集指令指针值。这些示例大致衡量了哪部分代码对应用程序执行的贡献。每一个足够的CPU周期都记录一个样本。注意，这必须只考虑相应跟踪任务所花费的CPU周期。我们使用硬件性能计数器和性能计数器溢出来采样[17]。主机维护一个变量counts_to_sample——在对当前运行的任务进行采样之前要执行的CPU周期数。这个变量在VM E NTRY, VM E XIT期间被更新。由于我们可能一次采样多个任务，counts_to_sample变量必须在C ONTEXT的切换事件期间保存和恢复。下面的步骤说明抽样程序。

在上课期间，

如果current_lwpid为NULL，那么我们不需要对当前任务进行采样。

•如果current_lwpid不是NULL，则需要计算CPU周期。性能计数器预先设置了一个值，以便在counts_to_sample CPU周期之后溢出。在硬件中启用虚拟机后进行计数。

在VM E退出期间，

•如果计数器溢出，创建一个充足的事件，事件信息作为当前指令指针值，并将counts_to_sample值重置为PLE。

否则，通过减去VM E NTRY和VM E XIT之间的CPU周期数来更新counts_to_sample。

另外，

•在处理事件期间，为了开始对新线程进行第一次采样，counts_to_sample变量被设置为足够的值。

•

在C ONTEXT年代女巫事件从任务P Q,如果P tracked_lwpids,然后保存在P lwpid counts_to_sample对象的当前值,在tracked_lwpids如果问,然后恢复值Q counts_to_lwpid lwpid对象

一旦收集了样本，工具就会为相应的功能聚合收集的样本。将原始地址映射到函数名之后，会显示结果。

5. EXPERIMENTAL EVALUATION

在本节中，我们将解释为评估应用程序诊断框架的效率而进行的各种实验。作为评估的一部分，我们将重点放在三个主要方面，(i)以经验验证记录和回放底层的正确性(ii)由于记录和回放底层以及事件通知和日志，开销的量化，以及(iii)展示我们的框架和工具的有用性。

#### 5.1 Record and replay correctness

InSight的一个重要特性是忠实地重新执行虚拟机，以利用执行重现性特性。为了验证InSight的正确性，我们进行了以下测试:

•我们在记录会话中运行top命令。在重放top命令时，我们比较了两个输出。作为顶部输出的一部分显示的所有字段都是相同的。

我们执行了一个CPU密集型的工作负载(Syn_cont，在5.3节中描述)，并比较了各种执行参数——执行时间、上下文切换数量、小的和大的页面错误、IO等待数量和交换页面数量。应用程序记录和回放阶段的所有参数都是相同的。

我们还验证了InSight的网络分组记录和DMA记录实现的正确性。为了测试正确的网络重播，我们设置了两个UDP客户端，向在虚拟机(为重播而记录)内执行的接收器发送数据包。接收方将所有数据包记录到一个文件中，我们在记录和回放期间验证了文件的校验和是相同的。选择UDP客户端背后的原因是有不确定的包的混合。为了验证DMA实现，我们使用dd命令创建了一个文件，输入文件为/dev/urandom。文件的校验和在记录和重播阶段是相同的。该测试还表明，系统的随机数生成器在重放期间是确定的。

#### 5.2 Overheads related to recording

InSight的设计目标之一是在记录阶段引入最小化开销，一个属性确保记录阶段的执行类似于真实世界的执行。我们进行了实验来确定记录阶段的开销。用于这些实验的系统具有以下特点:Intel Core i5处理器(2.8 GHz, 4核，4 GB RAM)。给虚拟机分配了1个vcpu和1.5 GB RAM。客户机和主机内核是相同的—linux 2.6.38.8，主机使用QEMU 0.14.0进行设备模拟。

我们的实验侧重于度量基准测试的完成时间以及对网络利用率的影响。我们尝试测量对磁盘IO的影响，但是由于页面缓存的多个级别(在主机和来宾服务器上)，我们无法得到任何决定性的结果。

三种不同的工作负载被用来估计开销,(我)W1: 10迭代程序的CPU密集型计算(计算的反正切值与公元前3000使用公用事业规模),(2)W2: 1000万rdtsc迭代的一个程序执行指令,和(3)W3: 5次迭代的一个程序从主机转移100 MB的客人。

表1报告了我们的实验结果。对于CPU密集型工作负载W1，记录开销可以忽略不计(小于1%)，所有执行都在接近8.7秒的持续时间内完成。我们对rdtsc指令使用软件仿真来启用记录和回放。这个陷阱和仿真模型引入的开销由W2工作负载评估。如果超过1000万次这样的调用，执行开销就会大大降低(大约降低150倍)。我们的工作负载是一个极端的场景，在现实中，这个开销分摊在很长一段时间内(通常100万条rdtsc指令分散在10分钟的执行中)。工作负载W3用于量化对网络利用率的影响。实现的网络带宽减少约22%。减少是由于记录包内容(为了重播)，这需要每秒钟写多个文件。另外，对于跨机器(或网络)的网络连接端点的情况，我们认为记录开销的影响会更小。

5.3 Diagnosis overheads

我们使用以下工作负载来评估InSight框架。前两个工作负载是基于pthread库的，其余的只有一个执行线程。

RUBiS是一个使用web服务器和数据库服务器的拍卖网站原型。我们使用Apache作为基于线程的多处理模块(httpd.worker)。选择基于线程的Apache来演示涉及多个基于线程的事件的应用程序的开销特征。web服务器在虚拟机中执行，并且web服务器的所有进程都被诊断。数据库服务器和客户机在不同的物理机器上执行。

•

•Syn_cont是一个模拟锁争用的合成工作负载，它有16个线程，每个线程执行以下代码。

```
void start_thread(int id) { for(i=0;i <1000;i++) { comp (); lock(level1[id /2]); comp () ; unlock(level1[id /2]); comp (); lock(level2[id /8]); comp () ; unlock(level2[id /8]); } }
```

comp()是一个CPU密集型函数，它模拟线程的活动。Syn_cont工作负载有两个级别的锁争用，级别1的锁在两个线程之间共享，级别2的锁在八个线程之间共享。

排序工作负载按字典方式对750 MB ASCII文件的行进行排序。输入文件存储在客户的本地存储中。

•

awk工作负载使用一个关联数组来计算上述文件中的记录数。

•sed工作负载替换了上述文件中的特定字符串模式。字符串匹配是使用正则表达式完成的。

下面的结果显示应用程序特征如何影响重播速度和事件数据库大小。表2给出了在不记录的情况下，使用显式和隐式事件捕获进行记录和回放的工作负载在不同执行场景下的执行时间和事件数据库大小。隐式采样事件，每X个CPU周期生成一个示例事件(表2中表示为ReplayX)。执行时间是从VM引导到VM关闭。sort和Syn_cont工作负载在VM关闭之前分别重复执行5次，而RUBiS工作负载在关闭之前处理10500个请求。

结果表明，重播速度与采样率成反比，事件数据库的大小由多个事件决定。排序工作负载是CPU和IO密集型的，不生成任何锁事件。较高的隐式事件抽样率(每100K周期抽取1个样本)生成了1279 MB的事件数据库——其中99%都是充足的事件。禁用隐式样本事件时，事件数据库的大小仅为5.8 MB，将每个样本的采样周期从100K更改为800K，更改排序工作负载的重播时间从≈40分钟减少到≈26分钟。Syn_cont工作负载是CPU密集型的，其结果与观察到的排序工作负载类似。

当工作负载不是CPU密集型时，重放速度高于记录速度。这是由于在重播期间跳过了停止指令——cpu不再等待来自重播日志的外部中断。RUBiS工作负载只有2%的CPU利用率，并且大部分都在等待数据库的响应。由于数据库响应是从重播日志中提供的，因此重播速度要快2倍。RUBiS工作负载的CPU利用率非常低，因此生成的充足事件的数量也很低——46 mb的32.5%——增加样本间隔对重播速度和事件数据库大小的影响微乎其微。

#### 5.4 Diagnosis results

在本节中，我们将展示展示诊断工具的有用性和正确性的结果。

##### 5.4.1 Lock usage analysis

为了评估锁使用工具的有用性，我们分析了Sys_cont和RUBiS工作负载的事件跟踪。表3总结了取得的结果。对于Syn_cont，这些值是5次运行的平均值，在每次运行中，这些值是相同级别锁的平均值。对于RUBiS，这些值是5次运行的平均值，而在每次运行中，这些值是Apache创建的5个线程组的平均值。

正如预期的那样，锁使用工具将Syn_cont工作负载中存在的两个2级锁识别为满足程度最高的(通过块的数量和块的%来度量)和共享程度最高的(通过两次连续的锁获取之间所有权变化的数量来度量)。在使用Syn_cont工作负载时，2级锁的争用和所有权变化更大，这两个锁的阻塞增加了10倍。我们通过使用相同的工作负载和mutrace工具[15]来验证这一点，该工具报告了类似的数字。对于RUBiS的工作量，没有锁的争夺，只有所有权的变更。我们将锁地址与Apache 2.4的源代码进行了匹配，并找到了最上面的两个锁(根据所有权变更的数量)。

##### 5.4.2 Function proﬁler

本节介绍功能分析诊断工具的结果。我们将结果与perf[18]工具进行了比较，以进行验证。我们在InSight框架内执行一组示例应用程序进行分析。perf在主机上使用相同的应用程序以CPU周期作为事件执行(我们用于实现的KVM版本不支持硬件计数器的虚拟化)。表4所示的结果是不同应用程序的5次不同运行的合并视图。通过基于InSight和基于perf的函数分析，我们给出了前4个函数的排名范围和样本百分比的平均值。我们的结果与perf结果非常一致。准确性是通过比perf-InSight的默认配置更多的样本来实现的，该配置以100K CPU周期作为样本间隔，收集了大约14到34倍的样本。InSight profiler报告了Syn_cont和RUBiS的线程级统计数据。与合并在所有线程上的perf报告相比，这些线程级报告更有用。

##### 5.4.3 Potential deadlock analysis

本节将展示为演示潜在死锁分析工具的使用而进行的实验。在一个成熟的程序中找到一个潜在的死锁是一项困难的任务。我们尝试了各种基于pthread的应用程序(evince、gedit、vlc和httpd)，但是没有发现任何潜在的死锁。我们开发了以下带有各种潜在死锁场景的示例应用程序，以验证我们工具的结果。这些示例应用程序可能会出现死锁，但是在执行期间不会发生死锁(我们在锁定事件之间引入了睡眠来序列化线程)。

```
Application 1: Trylock T1(){ if(trylock(L1) == acquired){ lock(L2); unlock(L2); unlock(L1); } } T2() { lock(L2); lock(L1); unlock(L1); unlock(L2); }

Application 2: Gatelock T1() { lock(Gate); lock(L1); lock(L2); unlock(L2); unlock(L1); unlock( Gate); } T2() { lock(Gate); lock(L2); lock(L1); unlock(L1); unlock(L2); unlock( Gate); }
```

对于这两个测试应用程序场景，我们的工具报告了正确的结果。潜在死锁工具识别Trylock和传递应用程序中的死锁。该工具没有报告任何死锁在Gatelock应用程序，但确定了一个潜在的死锁涉及锁L1和L2被锁闸保护。工具报告如下循环(名称被更改以匹配上面的示例。

•L2 T2→L1 T2→L1 T1→L2 T1→L2 T2, Trylock示例报告此周期。在本例中，我们确保trylock成功，以确保潜在的死锁。当trylock调用失败时，没有报告死锁。

•对于Gatelock应用，该工具报告了潜在周期L2 T2→L1 T2→L1 T1→L2 T1→L2 T1→L2 T2受锁闸保护。

这些应用程序的潜在死锁分析基于与诊断阶段捕获的锁相关的事件，上面的实验证明了它们的有用性。

### 6. RELATED WORK

在本节中，我们将简要讨论与确定性重放相关的工作，以及利用确定性重放进行有效的应用程序调试和分析的工作。在[9,10,19]中讨论了各种虚拟机记录和重放解决方案。InSight的记录和重放实现改进了这些，以提供高效的DMA和网络流量处理。这些解决方案和InSight适用于单处理器场景，而像DoublePlay[20]和[21]这样的解决方案已经将记录和回放扩展到多处理器场景。

横切[8]使用虚拟机记录和回放来提取进程级回放日志，可以通过Valgrind[6]和其他诊断工具进行回放。横切为Valgrind增加了可重复性，但是这种解决方案不能处理多进程应用程序，也不能利用硬件计数器进行分析。例如，横切解决方案不能用于识别共享内存段中具有进程级信号量和竞争条件的潜在死锁。使用InSight，如果生成信号量和共享内存访问事件，就可以开发诊断这些问题的工具。x射线[14]采用了利用记录和回放功能的两阶段诊断方法。x射线的目标是识别性能问题并跟踪这些问题的根本原因。x射线使用延迟、CPU使用率、文件系统使用率等作为性能指标。x射线依赖于污点分析，在选定的性能指标上提供性能总结。例如，当应用程序处理输入请求时，x射线会为处理请求所通过的每个基本代码块分配性能成本。随后，它总结了分配的成本，以确定基本块的主要根本原因。可以扩展x射线来识别锁争用并提供分析信息，但是它不能识别潜在的死锁和竞争条件。可以在InSight框架上实现类似的解决方案。例如，x射线将成本分配给基本块的解决方案可以通过为系统调用插入事件探测来构建，并通过生成隐式事件来识别基本块的执行。

重量级的在线诊断工具(专为多线程应用程序设计的)在线检测应用程序代码并分析执行情况。Valgrind缺乏可重现性，并导致应用程序执行的高开销。Valgrind的分析仅限于进程的地址空间，它不能调试多进程应用程序。此外，Valgrind重新实现了pthread库，以获得对线程执行的控制。这限制了Valgrind框架的泛化，而在InSight中，扩展对其他多线程库的支持更容易——可以预先加载一组包装器调用来生成事件。Tralfamadore[23]收集关于操作系统执行的低级跟踪，并回答有趣的查询，以帮助理解执行流。Tralfamadore也可以在线工作，这会导致很高的开销，而且由于操作系统事件被记录下来，应用程序级诊断是不可能的。

另一方面，InSight是离线的，是应用程序感知的(意思是提取过程/线程上下文)，并可以重播一个记录的会话多次，以提取事件的多次迭代。PinPlay[5]提供了可偿还的调试框架，使用Pin[13]仪器，以使并行程序的记录和重放设施。为了调试，PinPlay通过GDB的远程存根协议使用GDB[1]接口。PinPlay提供了再现性，但缺乏对InSight提供的诊断工具的支持。

### 7 CONCLUSIONS AND FUTURE WORK

对多线程和多进程应用程序的有效诊断具有挑战性，InSight提供了一个可扩展的框架，可用于构建诊断工具。我们实现了一个轻量级的InSight离线诊断框架，它使用了记录和回放技术。洞察框架具有重要的特性，重现性和最小开销没有任何妥协的有效诊断。构建InSight工具是为了证明我们框架的效率和有用性。我们进行了各种实验来测量InSight引入的开销，得到的结果是准确的，并表明InSight引入的应用程序执行开销更少。

我们目前的InSight工具是为多线程相关问题而设计的，比如锁争用和潜在的死锁检测。这些工具中使用的原理可以扩展到与多进程相关的应用程序——特别是与进程间通信相关的问题。InSight的当前实现没有提供识别内存相关问题的工具，如竞争条件、未初始化的内存读取和堆分析，这些问题由Valgrind[6]解决。在[24]中讨论的技术可以借助隐式事件应用于InSight框架中。与内存访问相关的隐式事件可以使用虚拟化层中提供的附加页表生成。然后，工具可以使用这些内存访问事件来识别多线程和多进程环境中的竞争条件。

